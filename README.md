# Regress√£o Linear S√≠mples :blue_book:
## Implementa√ß√£o do M√©todo de M√≠nimos Quadrados e M√©tricas para Modelo de Regress√£o Linear com Python
O m√©todo dos m√≠nimos quadrados √© o mesmo que reta de regress√£o linear s√≠mples ou reta de ajuste.
Interessante estudar esse modelo porque temos aplica√ß√µes em Machine Learning ü¶æ.

![Imagem 1](/imgs/exemplo_gif.gif)



## M√©todo dos M√≠nimos Quadrados ‚úîÔ∏è

O m√©todo dos m√≠nimos quadrados √© utilizado para ajustar um modelo de regress√£o linear aos dados observados, minimizando a soma dos quadrados dos res√≠duos (as diferen√ßas entre os valores observados e os valores preditos).
Para uma regress√£o linear simples com uma vari√°vel independente, a f√≥rmula dos coeficientes √©:

**Coeficiente Angular $A$**

$$ 
A = {\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) } / { \sum_{i=1}^{n} (x_i - \bar{x})^2 }
$$

**Coeficiente Linear $B$**

$$ B = \bar{y} - A \bar{x} $$

onde:
- **$x_i$**  s√£o os valores da vari√°vel independente,
- **$y_i$** s√£o os valores observados da vari√°vel dependente,
- **$\bar{x}$** √© a m√©dia dos valores da vari√°vel independente $$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$,
- **$\bar{y}$** √© a m√©dia dos valores da vari√°vel dependente $$\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$$,
- **$n$** √© o n√∫mero de observa√ß√µes.

### Equa√ß√£o da Regress√£o Linear
A equa√ß√£o da reta de regress√£o linear ajustada √© dada por:

$$ \hat{y} = B + A x $$

onde:
- **$\hat{y}$** √© o valor predito da vari√°vel dependente,
- **$A$** √© o coeficiente angular,
- **$B$** √© o coeficiente linear,
- **$x$** √© o valor da vari√°vel independente.

Usaremos essas f√≥rmulas para calcular os coeficientes que melhor se ajustam aos dados, minimizando a soma dos quadrados das diferen√ßas entre os valores observados e os valores preditos ü§ì.

## M√©tricas Performance ‚úîÔ∏è

### Raiz do Erro Quadr√°tico M√©dio (RMSE)

A Raiz do Erro Quadr√°tico M√©dio (RMSE) √© uma m√©trica que mede a m√©dia dos erros ao quadrado entre os valores observados e os valores preditos (MSE). Ela tem a mesma unidade dos valores observados, o que facilita a interpreta√ß√£o.

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

onde:
- **$y_i$** s√£o os valores observados,
- **$\hat{y}_i$** s√£o os valores preditos pelo modelo, para cada **$y_i$**,
- **$n$** √© o n√∫mero de observa√ß√µes.

### Coeficiente de Determina√ß√£o R¬≤

O Coeficiente de Determina√ß√£o R¬≤ √© uma m√©trica que mede a propor√ß√£o da variabilidade dos dados que √© explicada pelo modelo de regress√£o. Ele varia entre 0 e 1, onde 1 indica um ajuste perfeito e 0 indica que o modelo n√£o explica a variabilidade dos dados.

$$ 
R¬≤ = 1 - { \sum_{i=1}^{n} (y_i - \hat{y_i})} / { \sum_{i=1}^{n} (y_i - \bar{y})^2 }
$$


onde:
- **$y_i$** s√£o os valores observados,
- **$\hat{y}_i$** s√£o os valores preditos pelo modelo,
- **$\bar{y}$** √© a m√©dia dos valores observados $$\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$$,
- **$n$** √© o n√∫mero de observa√ß√µes.

# Regress√£o Log√≠stica Bin√°ria :closed_book:

### A f√≥rmula geral para o modelo de regress√£o log√≠stica bin√°ria √©:

$$ P(Y = 1 \mid \mathbf{X}) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_nX_n)}} $$

onde:
- $P(Y = 1 \mid \mathbf{X})$ √© a probabilidade de o evento ocorrer (Y = 1) dado o vetor de caracter√≠sticas $ \mathbf{X} $.
- $ \beta_0 $ √© o intercepto do modelo.
- $ \beta_1 $, $ \beta_2 $, $ \ldots $, $ \beta_n $ s√£o os coeficientes das vari√°veis independentes $ X_1, X_2, \ldots , X_n $ (Vetor de caracter√≠sticas).
- $e$ √© a base do logaritmo natural (aproximadamente 2.71828).

### Forma Logit

Podemos reescrever a equa√ß√£o na forma logit, que √© a fun√ß√£o log-odds (log das chances):

$$ \text{logit}(P(Y = 1 \mid \mathbf{X})) = \ln\left(\frac{P(Y = 1 \mid \mathbf{X})}{1 - P(Y = 1 \mid \mathbf{X})}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_nX_n $$

Nesta forma, a regress√£o log√≠stica se parece muito com a regress√£o linear, mas est√° modelando o log das chances do evento \(Y = 1\).

### Interpreta√ß√£o dos Coeficientes

- **$\beta_0$ (intercepto):** √â o log-odds de $Y = 1$ quando todas as vari√°veis $X_i$ s√£o $0$.
- **$\beta_i$ (coeficientes das vari√°veis):** Representa a mudan√ßa no log-odds de $Y = 1$ para uma unidade de mudan√ßa na vari√°vel $X_i$, mantendo todas as outras vari√°veis constantes.


